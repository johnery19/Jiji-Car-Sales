{"cells":[{"cell_type":"code","execution_count":null,"id":"a60fd8d7-9884-4c47-ac1f-c390456dc6ca","metadata":{"id":"a60fd8d7-9884-4c47-ac1f-c390456dc6ca"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import openpyxl"]},{"cell_type":"code","execution_count":1,"id":"4f201aa7-c608-4a49-8fb0-48e7c64922b6","metadata":{"id":"4f201aa7-c608-4a49-8fb0-48e7c64922b6","executionInfo":{"status":"error","timestamp":1726050466648,"user_tz":-60,"elapsed":428,"user":{"displayName":"Mike Adeyeye","userId":"03644509343696289962"}},"outputId":"39ddfe2f-2731-4d0d-d9f2-ce6f5cb8017f","colab":{"base_uri":"https://localhost:8080/","height":211}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'requests' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6524660f8ffe>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Number of pages to scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m  \u001b[0;31m# Adjust this based on how many pages you want to scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# List to hold scraped data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"]}],"source":["# Base URL of the Jiji cars page\n","base_url = \"https://jiji.ng/cars\"\n","\n","# Number of pages to scrape\n","num_pages = 100  # Adjust this based on how many pages you want to scrape\n","response = requests.get(url)\n","# List to hold scraped data\n","data = []\n","\n","# Loop through the specified number of pages\n","for page in range(1, num_pages + 1):\n","    # Construct the URL for the current page\n","    url = f\"{base_url}?page={page}\"\n","    #print(f\"Scraping page {page}: {url}\")\n","\n","    # Check if the request was successful\n","    if response.status_code == 200:\n","        # Parse the HTML content\n","        soup = BeautifulSoup(response.content, \"html.parser\")\n","\n","        # Find all car listing containers\n","        cars = soup.find_all(\"div\", class_=\"b-list-advert-base__data\")\n","\n","        # Loop through each car listing and extract details\n","        for car in cars:\n","            # Extract car title\n","            title_tag = car.find(\"div\", class_=\"b-list-advert-base__item-title\")\n","            car_title = title_tag.text.strip() if title_tag else \"N/A\"\n","\n","            # Extract car price\n","            price_tag = car.find(\"div\", class_=\"qa-advert-price\")\n","            car_price = price_tag.text.strip() if price_tag else \"N/A\"\n","\n","            # Extract car type\n","            type_tag = car.find(\"div\", class_=\"b-list-advert-base__item-attr\")\n","            car_type = type_tag.text.strip() if type_tag else \"N/A\"\n","\n","            # Extract the description to the individual car listing\n","            descr_tag = car.find(\"div\", class_=\"b-list-advert-base__description-text\")\n","            car_descr = descr_tag.text.strip() if descr_tag else \"N/A\"\n","\n","            # Append the extracted details to the data list\n","            data.append({\n","                    \"Title\": car_title,\n","                    \"Price\": car_price,\n","                    \"Type\": car_type,\n","                    \"Description\": car_descr\n","                })\n","        else: None\n","            #print(\"No car listings found on the page.\")\n","    else:\n","        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n","\n","# Convert the list of data into a pandas DataFrame\n","df = pd.DataFrame(data)\n","# Save the DataFrame to an Excel file\n","df.to_excel(\"jiji_data.xlsx\")"]},{"cell_type":"code","execution_count":null,"id":"f6051149-90de-4cf8-80be-d9722517951b","metadata":{"id":"f6051149-90de-4cf8-80be-d9722517951b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}